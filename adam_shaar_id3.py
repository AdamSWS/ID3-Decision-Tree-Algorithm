# -*- coding: utf-8 -*-
"""Adam_Shaar_id3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Snk8GzZFWkswj3x6yTnGvAN0X0_FluBR
"""

#Add your name here
#Name: Adam Shaar

"""First save a copy on this notebook in your Drive: File->Save a copy in Drive, then rename your file to firstname_lastname_id3.ipynb (e.g. pedram_rooshenas_id3.ipynb). Now you can start

Submission:

1- Run all cells (this is important, the results will remain there for us to look)

2- Download .ipynb

3- Download .py

4- Use Save as or Print to create a PDF version of the notebook  

5- Create a directory named: firstname_lastname_id3 (e.g. pedram_rooshenas_id3)

6- Put all three .ipynb, .py, and .pdf into the directory. (*** Don't forget the PDF and .py ***) 

7- Zip (don't use rar) and Submit on Gradescope
"""

import re
import pandas as pd
import math
import numpy as np

"""If you want to continue using Google Colab (strongly recommended), you can load the data to your Google Drive and mount it here

If you like to continue with your local machine, download the ID3.ipynb and run it on your local Jupyter.

"""

#Mounting Google Drive:
#After running this cell a popup window will appear and requesting to select your  Google account and give the access permission.
#You can either use your personal Google account or your UIC Google account.
from google.colab import drive
drive.mount('/content/gdrive')

#After successfull mount, you can browse your Google Drive using linux commands:
!ls /content/gdrive/

#I have placed the data under mlcourse/hw1/agaricus in my Google Drive, so the address is:
#Yours would be different!

!head /content/gdrive/MyDrive/mlcourse/hw1/agaricus/agaricuslepiotatest1.csv

#For this programs we are going to evaluate the ID3 on three different datasets:
#Change the path based on your directory address 
path="/content/gdrive/MyDrive/mlcourse/hw1/"


#for agaricus we don't have a separate validation set, so we are going to reuse the training set.
agaricus = ["agaricus/agaricuslepiotatrain1.csv",
              "agaricus/agaricuslepiotatrain1.csv",
              "agaricus/agaricuslepiotatest1.csv"]

dataset1 = ["data_sets1/training_set.csv",
            "data_sets1/validation_set.csv",
            "data_sets1/test_set.csv"]

# Load data from a file. It returns a list of data points as well as the list of variable names
def read_data(filename):
    f = open(filename, 'r')
    p = re.compile(',')
    data = []
    header = f.readline().strip()
    varnames = p.split(header)
    namehash = {}
    for l in f:
        data.append([int(x) for x in p.split(l.strip())])
    return (data, varnames)

dataset = agaricus
train_data, varnames = read_data(path+dataset[0])
#the last element in the list is the class value."

#You can transfer the data to Pandas dataframe or directly load it with pd.read_csv but manupulating python lists would be easier. 
#You can also use pandas to explore at the data.
data_df = pd.DataFrame(train_data, columns=varnames)
data_df

#We check the dimensions of our dataframe
data_df.shape

#The first number in the shape is the xnumber of rows and the second one is the number of columns
#so we can quickly find the number examples:
data_df.shape[0]

#The class label is often the last column
classlabel = data_df.columns[-1]
print(classlabel)

#e.g. number of instances with positive class
data_df['poisonous'].sum()

#First attribute:
attr1 = data_df.columns[0]
print(attr1)

#Finding the rows that cap-shape-bell is one:
data_df[data_df['cap-shape-bell'] == 1]

#Counting the number of rows that cap-shape-bell is one
(data_df['cap-shape-bell'] == 1).sum()

#Rows that have both  cap-shape-bell==1 and are population-several
data_df[(data_df['cap-shape-bell'] == 1) & (data_df['population-several'] == 1)]

#Number of rows: 
((data_df['cap-shape-bell'] == 1) & (data_df['population-several'] == 1)).sum()

#Lets drop cap-shape-bell column:
new_data_df = data_df.drop(columns='cap-shape-bell');
new_data_df.columns[0] #the first attribute is different now 
#also note that dataframe operations are immutable and you have to assign the output to a variable.

#Ok at this point I guess you know enough pandas to write the actual code.
#the main part that you gonna use dataframe is inside the build_tree function to split the data for each branch and
#in select_attr to do the required counting

#Here we have the neccessary data structure and auxilary functions for construcing the decision trees
#You don't need to modify the codes in this cell
class Node:
    """ Node class for a decision tree. """
    #var is the variable name for the node
    def __init__(self, var):
        self.var = var

    def classify(x):
        """ Handled by the subclasses. """
        return None

    def dump(self, indent):
        """ Handled by the subclasses. """
        return None


class Leaf(Node):
    #value is the label of the leaf node
    def __init__(self, value):
        Node.__init__(self, None);
        self.value = value

    def classify(self, x):
        return self.value

    def dump(self, indent):
        print(' %d' % self.value)


class Split(Node):
    #var: the variable that we create the split on
    #left and right are the branches for each side which are Nodes
    def __init__(self, var, left, right):
        Node.__init__(self, var)
        self.left = left
        self.right = right

    def classify(self, x):
        if x[self.var] == 0:
            return self.left.classify(x)
        else:
            return self.right.classify(x)
    
    #use to print out the tree recursively
    def dump(self, indent):
        if indent > 0:
            print('')
        for i in range(0, indent):
            print('| ', end='')
        print('%s = 0 :' % self.var,end='')
        self.left.dump(indent+1)
        for i in range(0, indent):
            print('| ', end='')
        print('%s = 1 :' % self.var,end='')
        self.right.dump(indent+1)

"""Helper function computes entropy of Bernoulli distribution with parameter p"""

def entropy(p):
  if p == 0 or p == 1:
    return 0
  ent = -1 * p * math.log(p) - (1 - p) * math.log(1 - p)
  return ent

"""Compute information gain for a particular split, given the counts 

ny_nxi : number of occurences of y=1 with x_i=1 for all i=1 to n (#y=1 $\wedge$ #xi =1)
(n is number of variables and each variable is binary)

nxi : number of occurrences of x_i=1 (#x_i)

ny : number of ocurrences of y=1

total: total number instances in this branch

"""

def infogain(ny_nxi, nxi, ny, total):
    return entropy(nxi / total) - (ny / total) * entropy(ny_nxi / ny)

#split_df: the portion of the dataframe that routed to the current branch
def select_attr(split_df):
  tarvar = split_df.columns[-1]
  maxgain = 0
  selected_attr = ""
  total = split_df.iloc[:,-1].count()
  for i in split_df.columns[:-1]:
    ny_nxi = ((split_df[i] == 1) & (split_df[tarvar] == 1)).sum()
    nxi = (split_df[i] == 1).sum()
    ny = (split_df[tarvar] == 1).sum()
    gain = infogain(ny_nxi, nxi, ny, total);
    if gain > maxgain:
      maxgain = gain
      selected_attr = i
  return (selected_attr, maxgain)

"""Build tree in a top-down manner, selecting splits until we hit a pure leaf or all splits look bad.

"""

#Recursive function for building the tree. Note that the vanilla ID3 stops when the nodes are pure (base condition for the recursion);
#config is potential dictionary of hyperparams that you can tune over validation dataset
#For potential list of hyperparams check here:
#https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier

def build_tree(data, config={}):
    tarvar = data.iloc[:, -1]
    if len(tarvar.unique()) == 1:
      return Leaf(tarvar.iloc[0])
    attr = select_attr(data)[0]
    node = Split(attr, None, None)
    for i in [0, 1]:
      split_df = data[data[attr] == i]
      if split_df.empty:
          child = Leaf(attr)
      else:
          child = build_tree(split_df)
      if i == 0:
        node.left = child
      else:
        node.right = child
    return node

"""Build the decision tree"""

model = build_tree(data_df)

model.dump(0)

"""Calcuating the accuracy"""

#Assume the last column in the class label and returns one dataframe for labels and one for attributes
def separate_attributes_label(data):
  class_column = data.columns[-1]
  xdata = data.drop(columns=class_column);
  ydata = data[class_column];
  return xdata, ydata

xtrain, ytrain = separate_attributes_label(data_df);

def accuracy(model, xdata, ydata):
  correct = 0.0;
  for i in range(xdata.shape[0]):
    if model.classify(xdata.loc[i]) == ydata.loc[i]:
      correct += 1;
  return correct / xdata.shape[0]

print("Train Accuracy: {}".format(accuracy(model, xtrain, ytrain)))



"""Now that we have all pieces working we can automate the training for any given dataset"""

def evaluate(dataset, config={}):
  train = pd.read_csv(path+dataset[0])
  val = pd.read_csv(path+dataset[1]) 
  test = pd.read_csv(path+dataset[2]) 

  xtrain, ytrain = separate_attributes_label(train);
  xval, yval = separate_attributes_label(val);
  xtest, ytest = separate_attributes_label(test);

  model = build_tree(train, config)
  model.dump(0)

  print("Train Accuracy: {}".format(accuracy(model, xtrain, ytrain)))
  print("Val Accuracy: {}".format(accuracy(model, xval, yval)))
  print("Test Accuracy: {}".format(accuracy(model, xtest, ytest)))

evaluate(agaricus)

config={}
evaluate(dataset1, config)